{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "from wfdb import processing\n",
    "\n",
    "from gc import collect as collect_garbage\n",
    "from psutil import virtual_memory\n",
    "from os import scandir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder = \"D:/nicu-hrv-data/00-deidentified-raw-waveforms/\"\n",
    "folder = \"C:/Users/aidan/Box/Deidentified-Raw-Waveforms/\"\n",
    "coldict = {\n",
    "    \"raw_waves_data_1a.csv\": [\"time\", \"257\"], \"raw_waves_data_1b.csv\": [\"time\", \"257\", \"258\"], \"raw_waves_data_1c.csv\": [\"time\", \"257\", \"258\"], \"raw_waves_data_1d.csv\": [\"time\", \"257\", \"258\", \"317\"], \n",
    "    \"raw_waves_data_1e.csv\": [\"time\", \"258\"],\n",
    "\n",
    "    \"raw_waves_data_2a.csv\": [\"time\", \"257\", \"258\"], \"raw_waves_data_2b.csv\": [\"time\", \"258\"], \"raw_waves_data_2c.csv\": [\"time\", \"257\"], \"raw_waves_data_2d.csv\": [\"time\", \"257\", \"258\"], \n",
    "    \"raw_waves_data_2e.csv\": [\"time\", \"257\", \"258\"],\n",
    "\n",
    "    \"raw_waves_data_3a.csv\": [\"time\", \"258\"], \"raw_waves_data_3b.csv\": [\"time\", \"258\"], \"raw_waves_data_3c.csv\": [\"time\", \"258\"], \"raw_waves_data_3d.csv\": [\"time\", \"258\"], \n",
    "    \"raw_waves_data_3e.csv\": [\"time\", \"257\", \"258\", \"317\"],\n",
    "\n",
    "    \"raw_waves_data_4a.csv\": [\"time\", \"257\", \"258\"], \"raw_waves_data_4b.csv\": [\"time\", \"257\", \"258\"], \"raw_waves_data_4c.csv\": [\"time\", \"257\"], \"raw_waves_data_4d.csv\": [\"time\", \"257\", \"258\"], \n",
    "    \"raw_waves_data_4e.csv\": [\"time\", \"257\", \"258\"],\n",
    "\n",
    "    \"raw_waves_data_5a.csv\": [\"time\", \"258\"], \"raw_waves_data_5b.csv\": [\"time\", \"258\"], \"raw_waves_data_5c.csv\": [\"time\", \"258\"], \"raw_waves_data_5d.csv\": [\"time\", \"258\", \"317\"],\n",
    "    \"raw_waves_data_5e.csv\": [\"time\", \"258\"],\n",
    "\n",
    "    \"raw_waves_data_6a.csv\": [\"time\", \"257\", \"258\"], \"raw_waves_data_6b.csv\": [\"time\", \"258\"], \"raw_waves_data_6c.csv\": [\"time\", \"258\"], \"raw_waves_data_6d.csv\": [\"time\", \"258\"], \"raw_waves_data_6e.csv\": [\"time\", \"258\"],\n",
    "    \n",
    "    \"raw_waves_data_7a.csv\": [\"time\", \"257\", \"258\"], \"raw_waves_data_7b.csv\": [\"time\", \"258\"], \"raw_waves_data_7c.csv\": [\"time\", \"258\"], \"raw_waves_data_7d.csv\": [\"time\", \"257\", \"258\", \"317\"], \n",
    "    \"raw_waves_data_7e.csv\": [\"time\", \"258\"]\n",
    "}\n",
    "\n",
    "namedict = {\n",
    "    \"raw_waves_data_1a.csv\": \"1a\", \"raw_waves_data_1b.csv\": \"1b\", \"raw_waves_data_1c.csv\": \"1c\", \"raw_waves_data_1d.csv\": \"1d\", \"raw_waves_data_1e.csv\": \"1e\",\n",
    "    \"raw_waves_data_2a.csv\": \"2a\", \"raw_waves_data_2b.csv\": \"2b\", \"raw_waves_data_2c.csv\": \"2c\", \"raw_waves_data_2d.csv\": \"2d\", \"raw_waves_data_2e.csv\": \"2e\",\n",
    "    \"raw_waves_data_3a.csv\": \"3a\", \"raw_waves_data_3b.csv\": \"3b\", \"raw_waves_data_3c.csv\": \"3c\", \"raw_waves_data_3d.csv\": \"3d\", \"raw_waves_data_3e.csv\": \"3e\",\n",
    "    \"raw_waves_data_4a.csv\": \"4a\", \"raw_waves_data_4b.csv\": \"4b\", \"raw_waves_data_4c.csv\": \"4c\", \"raw_waves_data_4d.csv\": \"4d\", \"raw_waves_data_4e.csv\": \"4e\",\n",
    "    \"raw_waves_data_5a.csv\": \"5a\", \"raw_waves_data_5b.csv\": \"5b\", \"raw_waves_data_5c.csv\": \"5c\", \"raw_waves_data_5d.csv\": \"5d\", \"raw_waves_data_5e.csv\": \"5e\",\n",
    "    \"raw_waves_data_6a.csv\": \"6a\", \"raw_waves_data_6b.csv\": \"6b\", \"raw_waves_data_6c.csv\": \"6c\", \"raw_waves_data_6d.csv\": \"6d\", \"raw_waves_data_6e.csv\": \"6e\",\n",
    "    \"raw_waves_data_7a.csv\": \"7a\", \"raw_waves_data_7b.csv\": \"7b\", \"raw_waves_data_7c.csv\": \"7c\", \"raw_waves_data_7d.csv\": \"7d\", \"raw_waves_data_7e.csv\": \"7e\"    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=12655771648, available=5600448512, percent=55.7, used=7055323136, free=5600448512)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_garbage()\n",
    "virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [folder + file.name for file in scandir(folder) if \".csv\" in file.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a file name, gets the key needed to index the above dictionaries\n",
    "def get_key(file_name):\n",
    "    return file_name.split(\"/\")[-1]\n",
    "\n",
    "# Takes a dataframe of ECG signals, completes the signals, fill in missing data with ffill, \n",
    "# and returns a dataframe containing the new signal\n",
    "def complete_signal(df):\n",
    "    cols = df.columns\n",
    "    signal = pd.Series(df[cols[1]])\n",
    "    i=2\n",
    "    while True:\n",
    "        try:\n",
    "            signal = signal.combine_first(df[cols[i]])\n",
    "        except IndexError:\n",
    "            break\n",
    "    signal = signal.fillna(method=\"ffill\")\n",
    "    signal = pd.to_numeric(signal)\n",
    "    return signal\n",
    "\n",
    "# Takes signal, and modifies it in place to have no spikes or troughs\n",
    "def remove_extrema(signal):\n",
    "    delta = 125\n",
    "    filt = (signal <= -10) | (signal >= 10)\n",
    "    filt.loc[~filt] = np.nan\n",
    "    filt.fillna(method=\"ffill\", limit=delta, inplace=True)\n",
    "    filt.fillna(method=\"bfill\", limit=delta, inplace=True)\n",
    "    filt.fillna(value=False, inplace=True)\n",
    "    \n",
    "    signal.loc[filt] = np.nan\n",
    "    signal.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "# Takes a signal and returns a list of qrs locations, as found by XQRS\n",
    "def detect_qrs(signal, freq=250, chunk=10000):\n",
    "    # Initialize the rpeak list\n",
    "    rpeaks = []\n",
    "\n",
    "    # Create a counter for breaking the signal into chunks\n",
    "    i=0\n",
    "    N = len(signal)\n",
    "    chunk = 10000\n",
    "    num_chunks = N//chunk + 1\n",
    "\n",
    "    # Find R peaks in all but the last chunk (that just tends to cause a problem)\n",
    "    while True:\n",
    "        try:\n",
    "            if i%1000 == 0:\n",
    "                # I've found this choice of progress marker works for this chunk\n",
    "                # size and signal length. If those values change, then this \n",
    "                # condition will need to be modified too\n",
    "                print( str(round(i/num_chunks,4)*100) + \"% percent done\" )\n",
    "\n",
    "            lo = i*chunk\n",
    "            hi = min( (i+1)*chunk, N)\n",
    "            xqrs = processing.XQRS(sig=signal[lo:hi], fs=freq)\n",
    "            xqrs.detect(verbose=False)\n",
    "\n",
    "            # xqrs recognized the chunk as starting from 0, so we have to shift \n",
    "            # the R peaks according to the left endpoint of the chunk\n",
    "            rpeaks += list( lo + xqrs.qrs_inds )\n",
    "\n",
    "            i+=1\n",
    "        except IndexError:\n",
    "            # This is the main way in which we'd expect to break this loop\n",
    "            break\n",
    "        except ValueError:\n",
    "            # More often than not, we get this case because the last chunk isn't \n",
    "            # long enough, hence the next block\n",
    "            break\n",
    "    print(\"R peaks outside of the last chunk located\")\n",
    "\n",
    "    # Delineate an ending chunk of like 20000 indices that gets the end of the\n",
    "    # signal, find R peaks\n",
    "    hi = len(signal)\n",
    "    lo = hi - 20000\n",
    "    xqrs = processing.XQRS(sig=signal[lo:hi], fs=freq)\n",
    "    xqrs.detect(verbose=False)\n",
    "\n",
    "    rpeaks += [ peak for peak in xqrs.qrs_inds if peak > max(rpeaks)]\n",
    "\n",
    "    return rpeaks\n",
    "\n",
    "# Removes RR intervals that are too small and returns a new dataframe with \n",
    "# RR intervals greater than that threshold\n",
    "def remove_too_small(df, lo=0.25):\n",
    "    filt = df[\"interval\"] >= lo\n",
    "\n",
    "    new_df = pd.DataFrame.copy(df.loc[filt], deep=True)\n",
    "    new_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    new_df[\"interval\"] = new_df[\"time\"].diff()\n",
    "\n",
    "    return new_df\n",
    "\n",
    "# Removes RR intervals that are too big (inplace), just treats them as missing data\n",
    "def remove_too_big(df, hi=5):\n",
    "    filt = df[\"interval\"] <= 5\n",
    "    df = df.loc[filt]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "# Breaks up any multiple intervals\n",
    "def remove_multiple_intervals(df):\n",
    "    # I have the code for this at home and can implement it when I get there\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting now with file raw_waves_data_1a.csv\n",
      "55.8% memory usage\n",
      "Raw data loaded in\n",
      "58.1% memory usage\n",
      "Signal extracted from raw data\n",
      "60.9% memory usage\n",
      "0.0% percent done\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,8):\n",
    "    files = [file for file in all_files if \"_\"+str(i) in file]\n",
    "\n",
    "    peak_df = None\n",
    "    for file in files:\n",
    "        # Preliminaries\n",
    "        key = get_key(file)\n",
    "        cols = coldict[key]\n",
    "        freq = 250\n",
    "        print(\"Starting now with file \" + key)\n",
    "        print(str(virtual_memory()[2]) + \"% memory usage\")\n",
    "\n",
    "        # Read in the data\n",
    "        df = pd.read_csv(file, usecols=cols)\n",
    "        print(\"Raw data loaded in\")\n",
    "        print(str(virtual_memory()[2]) + \"% memory usage\")\n",
    "        \n",
    "        # Get a workable signal\n",
    "        signal = complete_signal(df)\n",
    "        remove_extrema(signal)\n",
    "        print(\"Signal extracted from raw data\")\n",
    "        print(str(virtual_memory()[2]) + \"% memory usage\")\n",
    "\n",
    "        # Detect QRS complexes\n",
    "        rpeaks = detect_qrs(signal, freq=250, chunk=10000)\n",
    "        print(\"QRS complexes located\")\n",
    "        print(str(virtual_memory()[2]) + \"% memory usage\")\n",
    "\n",
    "        if peak_df==None:\n",
    "            peak_df = df.loc[rpeaks, \"time\"]\n",
    "        else:\n",
    "            peak_df = peak_df.append(df.loc[rpeaks, \"time\"])\n",
    "        del df\n",
    "        collect_garbage()\n",
    "\n",
    "    peak_df.reset_index(inplace=True)\n",
    "\n",
    "    # Write the peaks to an output file\n",
    "    peak_df.to_csv(\"D:/nicu-hrv-data/01-rpeaks/rpeaks_\" + str(i) + \".csv\", index=False)\n",
    "\n",
    "    # Clean up\n",
    "    del peak_df\n",
    "    collect_garbage()\n",
    "    print(\"Done with infant \" + str(i))\n",
    "    print(str(virtual_memory()[2]) + \"% memory usage\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag any beats that are < 0.25 seconds, remove them, and recalculate the RR intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALRIGHT, PART ONE IS DONE, NOW I NEED TO FIX THE RR INTERVALS TO REMOVE DOUBLE INTERVALS, WILL ALSO TAKE A WHILE\n",
    "# Load in the rpeaks for each infant\n",
    "for i in range(2,8):\n",
    "    print(\"Starting now with infant \" + str(i))\n",
    "    # Get the names of the rpeak files\n",
    "    rpeak_files = sorted([\"07-pipeline-outputs/01-rpeaks/rpeaks_\"+str(i)+part+\".csv\" for part in [\"a\", \"b\", \"c\", \"d\", \"e\"]])\n",
    "    \n",
    "    # Concatenate all of these dataframes\n",
    "    df = pd.read_csv( rpeak_files.pop(0) )\n",
    "    for file in rpeak_files:\n",
    "        df = df.append( pd.read_csv( file ) )\n",
    "        collect_garbage()\n",
    "    print(\"Infant \" + str(i) + \" R-peaks all loaded\")\n",
    "    \n",
    "    # Clean up df, calculate the RR intervals, and write the results just in case\n",
    "    df.drop(\"Unnamed: 0\", inplace=True, axis=1)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df[\"interval\"] = df[\"time\"].diff()\n",
    "    df.to_csv(\"07-pipeline-outputs/02-dirty-rr-intervals/dirty-rr-intervals_\"+str(i)+\".csv\", index=False)\n",
    "    print(\"Unprocessed RR Intervals Written\")\n",
    "\n",
    "    # Recalculate RR intervals while ignoring beats whose RR intervals are > 0.25 seconds\n",
    "    filt = df[\"interval\"] >= 0.25\n",
    "\n",
    "    new_df = pd.DataFrame.copy(df.loc[filt], deep=True)\n",
    "    new_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    new_df[\"interval\"] = new_df[\"time\"].diff()\n",
    "\n",
    "    del df\n",
    "    collect_garbage()\n",
    "    df = pd.DataFrame.copy(new_df, deep=True)\n",
    "    del new_df\n",
    "    collect_garbage()\n",
    "\n",
    "    # Remove intervals of length greater than 5 seconds (this is arbitrary)\n",
    "    # The resulting gaps will just be treated as missing data\n",
    "    filt = df[\"interval\"] <= 5\n",
    "    df = df.loc[filt]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    print(\"Intervals outside of [0.25, 10] dealt with\")\n",
    "\n",
    "    # Get rid of any remaining multiple intervals\n",
    "    df.set_index(df[\"time\"], inplace=True)\n",
    "    df.drop(\"time\", inplace=True, axis=1)\n",
    "\n",
    "    df_buffer = pd.DataFrame.copy(df, deep=True)\n",
    "\n",
    "    # THIS IS THE BULKY PART OF THIS PROCESS, AND IT DOES TAKE A LONG TIME (~1H PER INFANT).\n",
    "    # INVESTIGATION INTO OTHER METHODS WOULD BE WORTHWHILE\n",
    "    # Form a generator to iterate over the rows\n",
    "    rows = df_buffer.iterrows()\n",
    "    progress_chunk = len(df[\"interval\"]) // 20\n",
    "\n",
    "    # Get the initial values\n",
    "    prev_idx, prev_row = next(rows)\n",
    "    prev_ivl = prev_row[\"interval\"]\n",
    "\n",
    "    # A counter to see how many beats were imputed\n",
    "    imputed = 0\n",
    "    max_imputed = 0\n",
    "\n",
    "    # A counter for progress\n",
    "    counter = 0\n",
    "\n",
    "    for curr_idx, curr_row in rows:\n",
    "        if counter % progress_chunk == 0:\n",
    "            print( str(round( counter/progress_chunk, 4)*100) + \"% Complete\" )\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "        curr_ivl = curr_row[\"interval\"]\n",
    "\n",
    "        pieces = round(curr_ivl/prev_ivl)\n",
    "\n",
    "        if pieces >= 2: # then it is likely that the current interval is a multiple interval\n",
    "            fill_value = curr_ivl / pieces\n",
    "\n",
    "            while fill_value < 0.25: # We have too many pieces and the beats are unrealistically small (i.e. < 0.25 seconds)\n",
    "                pieces -= 1\n",
    "                fill_value = curr_ivl/pieces\n",
    "                if pieces == 1:\n",
    "                    break\n",
    "                \n",
    "            if pieces == 1: # Then there's not point in carrying on with this iteration, update previous values and continue\n",
    "                prev_idx = curr_idx\n",
    "                prev_ivl = curr_ivl\n",
    "                continue\n",
    "\n",
    "            imputed += pieces\n",
    "            max_imputed = max(max_imputed, pieces)\n",
    "\n",
    "            # Otherwise, we impute the RR intervals, modifying df and NOT df_buffer\n",
    "            endpoints = [prev_idx + i*fill_value for i in range(1, pieces)] + [curr_idx]\n",
    "            for t in endpoints:\n",
    "                df.loc[t,\"interval\"] = fill_value\n",
    "\n",
    "            # Now, we update the previous values ahead of the next iteration\n",
    "            prev_idx = curr_idx\n",
    "            prev_ivl = fill_value\n",
    "            continue\n",
    "\n",
    "        # If we didn't enter the pieces >= 2 case, then we need to update the previous values in a different way\n",
    "        prev_idx = curr_idx\n",
    "        prev_ivl = curr_ivl\n",
    "\n",
    "    print(\"Multiple intervals all broken up\")\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    df.to_csv(\"07-pipeline-outputs/03-clean-rr-intervals/rr_intervals_imputed_\"+str(i)+\".csv\")\n",
    "    del df\n",
    "    del df_buffer\n",
    "    del rows\n",
    "    collect_garbage()\n",
    "    print(\"Cleaned RR intervals written to file, \"+str(virtual_memory()[2])+\"% memory usage\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (tags/v3.9.2:1a79785, Feb 19 2021, 13:44:55) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b66384013171cdfccbfda8d4f96d2b3b6a10bf536bc42f61d1fe50046d47db0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
